---
title: "3_Linear_Regression"
author: "T.A. Meraxa"
date: "July 3, 2016"
output: html_document
---

### 3. Linear Regression

In R models are fitted using model-fitting function, that is `lm()`. Usage is as follows 
`fm <- lm(formula,data,..)`. Generic functions such as `summary()`,`residuals()`,or `predict()` are available for all standard models.

#### 3.1. Simple Linear Regression

```{r}
# Load data
data("Journals")
journals <- Journals[,c("subs","price")]
citeprice <- Journals$price/Journals$citations
```

```{r}
# see the data structure and summary
summary(Journals)
```

The goal is to estimate the effect of the price per citation on the number of library subscription. Therefore, we fit a linear regression model

log(subs)[i]= Beta[1] + Beta[2]log(citeprice)+Epsilon[i]

The primary function of tthe `lm()` estimates a linear regression using ordinary least squares

```{r}
# plot the regression fit
plot(log(subs) ~ log(citeprice), data = journals)
jour_lm <- lm(log(subs) ~ log(citeprice), data = journals)
abline(jour_lm)
```

```{r}
# check the class of fitted model jour_lm
class(jour_lm)

# check the names of its components
names(jour_lm)

# check the summary of jour_lm
summary(jour_lm)

# create the summary for the fitten lm object
jour_slm <- summary(jour_lm)
class(jour_slm)
names (jour_slm)

#check the coefficient of jour_slm
jour_slm$coefficients
```

**Useful functions**

* `print()`     simple printed display
* `summary()`   standard regression output
* `coef()`      (or `coefficients()`) extracting the regression coefficients
* `residuals()` (or `resid()`) extracting residuals
* `fitted()`    (or `fitted.values()`) extracting fitted values
* `anova()`     comparison of nested models
* `predict()`   predictions for new data
* `plot()`      diagnostic plots
* `confint()`   confidence intervals for the regression coefficients
* `deviance()`  residual sum of squares
* `vcov()`      (estimated) variance-covariance matrix
* `logLik()`    log-likelihood (assuming normally distributed errors)
* `AIC()`       information criteria including AIC, BIC/SBC (assuming normally distributed errors)

###### Analysis of Variance

```{r}
# perform analysis of variance
anova(jour_lm)
```

###### Point and interval estimates

```{r}
# extracting the estimated regression coefficients
coef(jour_lm)

# obtain confidence interval
confint(jour_lm, level = 0.95)
```

###### Prediction

```{r}
# predicting points on the regression line and predicting a new data value
predict(jour_lm, newdata = data.frame(citeprice = 2.11), interval = "confidence")
predict(jour_lm, newdata = data.frame(citeprice = 2.11), interval = "prediction")
        
#
lciteprice <- seq(from = -6, to = 4, by = 0.25)
jour_pred <- predict(jour_lm, interval = "prediction",
                     newdata = data.frame(citeprice = exp(lciteprice)))
```

###### Plotting "lm" objects

```{r}
# predicting points on the regression line and predicting a new data value
par(mfrow = c(2,2))
plot(jour_lm)
par(mfrow = c(1,1))
```

###### Testing linear hypothesis

```{r}
# linear hypothesis test
linearHypothesis(jour_lm, "log(citeprice) = -0.5")
```

#### 3.2. Multiple Linear Regression

In economics, most regression analyses comprise more than a single regressor. Often there are regressor of a soecial type, usually referred to as dummy variables in econometrics, which are used for coding categorical variables. 

Furthermore, it is also necessary to transform regressors or dependent variables.

Here, we employ the CPS1988 data frame collected in the March 1988 Current Population Survey (CPS) by the US Census Bureau and analyzed by Bierens and Ginther (2001). This is an industry strenght example.

```{r}
# load data
data("CPS1988")
summary(CPS1988)
attach(CPS1988)
```

The model is

log(wage)[i]= Beta[1] + Beta[2]experience + Beta[3]experience^2 + Beta[4]education + Beta[5] ethnicity + Epsilon[i]

```{r}
# fit the model in R
cps_lm <- lm(log(wage) ~ experience + I(experience^2) + education + ethnicity, data = CPS1988)

# review the model
summary(cps_lm)
```

##### Comparison of models

```{r}
# create a new regression without ethnicity
cps_noeth <- lm(log(wage) ~ experience + I(experience^2) + education, data = CPS1988)

# run ANOVA between the models
anova(cps_noeth, cps_lm)

# review ANOVA of one model
anova(cps_lm)
anova(cps_noeth)
```

##### Updating model

```{r}
# create a new regression without ethnicity
cps_noeth2 <- update(cps_lm, formula = .~ . - ethnicity)
summary(cps_noeth2)
```

##### Wald test

Wald test produces the same F test as `anova()` but does not report RSS. `waldtest()` can also perform quasi-F tests in situatiins where errors are potentially heteroskedastic.

```{r}
# create a new regression without ethnicity
waldtest(cps_lm, .~ . - ethnicity)
```

```{r}
# data prep
library(AER)
library(splines)
data("CPS1988")
attach(CPS1988)l
cps_lm <- lm(log(wage) ~ experience + I(experience^2) + education + ethnicity, data = CPS1988)
```


#### 3.3 Partly Linear Model

Quadrati terms in experience are common in wage equations; however, given the size of the CPS1988 data, it may be worthwile to model the role of thisvariable using more flexible tools.

Consider the model:

log(wage)= Beta[1] + g(experience) + Beta[2]education + Beta[3]ethnicity + Epsilon

Here, g is an unknown function to be estimated from the data. Regression splines are use for this task. Splines is included in the packages `("splines")` and also in `("AER")`. 

```{r}
# computing B splines
cps_plm <- lm(log(wage) ~ bs(experience, df = 5) + education + ethnicity, data = CPS1988)
summary(cps_plm)
```

The choice of `df = 5` is based on Schwarz criterion (BIC). See the code below.

```{r}
# computing B splines; Schwarz criterion (BIC) selection 
cps_bs <- lapply(3:10, function (i) lm(log(wage) ~
          bs(experience, df = i) + education + ethnicity,
          data = CPS1988))

structure(sapply(cps_bs, AIC, k = log(nrow(CPS1988))),
          .Names = 3:10)
```

The cubic spline selected from the model is comparred with the classical fit form `cps_lm`

```{r}
# plot a graph
cps <- data.frame(
    experience = -2:60, 
    education = with (CPS1988, mean(education[ethnicity == "cauc"])),
    ethnicity = "cauc")

cps$yhat1 <- predict(cps_lm, newdata = cps)
cps$yhat2 <- predict(cps_plm,newdata = cps)
plot(log(wage) ~ jitter(experience, factor = 3), 
     pch = 19,
     col = rgb(0.5, 0.5, 0.5, alpha = 0.02), 
     data = CPS1988)
lines(yhat1 ~ experience, data = cps, lty = 2)
lines(yhat2 ~ experience, data = cps)
legend("topleft", c("quadratic", "spline"), lty = c(2,1), bty = "n")

```

The spline are not too distinct for the 20-40 years of experience range. Overall, the spline version exhibits less curvature beyond eight years of experience. The highlight of this feature is that the more pronounced curvature below seven years of experience.